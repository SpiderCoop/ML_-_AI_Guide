{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arboles de decision Bayesianos\n",
    "\n",
    "El **Árbol de Decisión Bayesiano** es una extensión de los árboles de decisión tradicionales que incorpora principios de la estadística bayesiana para tomar decisiones en cada nodo. A diferencia de los árboles de decisión estándar, donde las decisiones se basan en divisiones determinísticas según una métrica como la ganancia de información o el índice de Gini, un árbol de decisión bayesiano asigna distribuciones de probabilidad a las características y calcula probabilidades condicionadas en cada nodo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estructura del Modelo\n",
    "\n",
    "1. **Nodos Internos**: En un árbol de decisión bayesiano, cada nodo interno representa una prueba o condición basada en una o más características del conjunto de datos. En lugar de realizar una división basada en un solo valor, se asigna una distribución de probabilidad a la característica relevante.\n",
    "\n",
    "2. **Distribución de Probabilidad**: Cada característica en un nodo puede tener una distribución de probabilidad asociada. Por ejemplo, si una característica es continua, podría estar modelada con una distribución normal (Gaussiana). Si es categórica, podría estar modelada con una distribución multinomial.\n",
    "\n",
    "3. **Hiperparámetros**: Los hiperparámetros son los parámetros que definen la distribución de probabilidad. En el caso de una distribución normal, los hiperparámetros serían la media y la varianza. Estos hiperparámetros pueden ser ajustados durante el entrenamiento del modelo.\n",
    "\n",
    "4. **Probabilidades Condicionadas**: En cada nodo, se calculan las probabilidades condicionadas de las diferentes clases, dado el valor de la característica. Estas probabilidades se actualizan utilizando el teorema de Bayes a medida que se recorre el árbol.\n",
    "\n",
    "5. **Clasificación Final**: Para clasificar una instancia, se recorre el árbol desde la raíz hasta una hoja, multiplicando las probabilidades a lo largo del camino. La clase con la mayor probabilidad final es asignada a la instancia.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo Matemático\n",
    "\n",
    "Consideremos un ejemplo simple con una característica continua $x$ y dos posibles clases $C_1$ y $C_2$.\n",
    "\n",
    "1. **Distribución de Probabilidad**: Supongamos que $x$ sigue una distribución normal:\n",
    "   $\n",
    "   P(x|C_i) = \\frac{1}{\\sqrt{2\\pi \\sigma_i^2}} \\exp\\left(-\\frac{(x - \\mu_i)^2}{2\\sigma_i^2}\\right)\n",
    "   $\n",
    "   donde $\\mu_i$ y $\\sigma_i^2$ son los hiperparámetros (media y varianza) para la clase $C_i$.\n",
    "\n",
    "2. **Probabilidad a Priori**: Se asignan probabilidades a priori a las clases, $P(C_1)$ y $P(C_2)$.\n",
    "\n",
    "3. **Probabilidad Posteriori**: Al observar un valor $x$, se calculan las probabilidades posteriores utilizando el teorema de Bayes:\n",
    "   $\n",
    "   P(C_i|x) = \\frac{P(x|C_i) P(C_i)}{P(x)}\n",
    "   $\n",
    "   donde $P(x) = \\sum_{i} P(x|C_i)P(C_i)$ es la probabilidad marginal de $x$.\n",
    "\n",
    "4. **Decisión**: Se asigna la clase con la mayor probabilidad posterior:\n",
    "   $\n",
    "   \\hat{C} = \\arg\\max_{i} P(C_i|x)\n",
    "   $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ventajas y Desventajas\n",
    "\n",
    "- **Ventajas**:\n",
    "  - Permite incorporar incertidumbre en las decisiones.\n",
    "  - Es más robusto ante la variabilidad en los datos.\n",
    "  - Puede manejar datos con ruido y datos faltantes de manera más efectiva.\n",
    "\n",
    "- **Desventajas**:\n",
    "  - Mayor complejidad computacional debido al cálculo de probabilidades en cada nodo.\n",
    "  - Requiere estimar y ajustar correctamente las distribuciones de probabilidad y sus hiperparámetros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicaciones\n",
    "\n",
    "Este modelo es útil en situaciones donde es importante capturar la incertidumbre en la toma de decisiones, como en sistemas de diagnóstico médico, detección de fraudes, y otras aplicaciones donde los riesgos asociados con decisiones incorrectas son altos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import norm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Simular datos\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "\n",
    "# Característica 1: Peso\n",
    "X1_class0 = np.random.normal(160, 10, n_samples // 2)\n",
    "X1_class1 = np.random.normal(180, 10, n_samples // 2)\n",
    "\n",
    "# Característica 2: Tamaño\n",
    "X2_class0 = np.random.normal(5, 1, n_samples // 2)\n",
    "X2_class1 = np.random.normal(7, 1, n_samples // 2)\n",
    "\n",
    "# Etiquetas\n",
    "y_class0 = np.zeros(n_samples // 2)\n",
    "y_class1 = np.ones(n_samples // 2)\n",
    "\n",
    "# Unir datos\n",
    "X = np.vstack((np.hstack((X1_class0, X1_class1)), np.hstack((X2_class0, X2_class1)))).T\n",
    "y = np.hstack((y_class0, y_class1))\n",
    "\n",
    "# Crear DataFrame\n",
    "df = pd.DataFrame(X, columns=['Peso', 'Tamaño'])\n",
    "df['Clase'] = y\n",
    "\n",
    "# Dividir en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianDecisionTree:\n",
    "    \"\"\"\n",
    "    Un árbol de decisión que utiliza un enfoque bayesiano para la clasificación.\n",
    "    Este modelo combina la estructura de un árbol de decisión con la estimación\n",
    "    de probabilidades basadas en distribuciones Gaussianas.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Inicializa el modelo como un diccionario vacío que contendrá la estructura del árbol.\"\"\"\n",
    "        self.model = {}\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Ajusta el modelo a los datos de entrenamiento.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Matriz de características de forma (n_samples, n_features).\n",
    "            y (numpy.ndarray): Vector de etiquetas de clase de forma (n_samples,).\n",
    "\n",
    "        \"\"\"\n",
    "        # La raíz del árbol se construye dividiendo los datos en nodos\n",
    "        self.model['root'] = self._split_node(X, y)\n",
    "\n",
    "    def _split_node(self, X, y):\n",
    "        \"\"\"\n",
    "        Encuentra la mejor característica y umbral para dividir los datos en un nodo.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Matriz de características.\n",
    "            y (numpy.ndarray): Vector de etiquetas de clase.\n",
    "\n",
    "        Returns:\n",
    "            dict: Nodo dividido con la característica seleccionada, umbral y\n",
    "                  las distribuciones de probabilidad condicional para cada rama (izquierda y derecha).\n",
    "        \"\"\"\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "        best_score = float('inf')  # Inicializa el mejor puntaje con infinito\n",
    "\n",
    "        # Itera sobre todas las características para encontrar la mejor\n",
    "        for feature_idx in range(X.shape[1]):\n",
    "            thresholds = np.unique(X[:, feature_idx])  # Umbrales únicos de la característica\n",
    "            for threshold in thresholds:\n",
    "                score = self._calculate_score(X, y, feature_idx, threshold)\n",
    "                if score < best_score:  # Busca minimizar el puntaje\n",
    "                    best_score = score\n",
    "                    best_feature = feature_idx\n",
    "                    best_threshold = threshold\n",
    "\n",
    "        # Divide los datos en dos subconjuntos: izquierdo y derecho\n",
    "        left_indices = X[:, best_feature] <= best_threshold\n",
    "        right_indices = X[:, best_feature] > best_threshold\n",
    "\n",
    "        # Crea el nodo con la mejor característica y umbral encontrados\n",
    "        left_node = {\n",
    "            'feature': best_feature,\n",
    "            'threshold': best_threshold,\n",
    "            'left': self._calculate_likelihoods(X[left_indices], y[left_indices]),\n",
    "            'right': self._calculate_likelihoods(X[right_indices], y[right_indices])\n",
    "        }\n",
    "\n",
    "        return left_node\n",
    "\n",
    "    def _calculate_score(self, X, y, feature_idx, threshold):\n",
    "        \"\"\"\n",
    "        Calcula un puntaje para una división específica basada en la suma de las probabilidades logarítmicas negativas.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Matriz de características.\n",
    "            y (numpy.ndarray): Vector de etiquetas de clase.\n",
    "            feature_idx (int): Índice de la característica seleccionada.\n",
    "            threshold (float): Umbral de la característica para la división.\n",
    "\n",
    "        Returns:\n",
    "            float: Puntaje basado en la suma de las probabilidades logarítmicas negativas.\n",
    "        \"\"\"\n",
    "        # Divide los datos según el umbral\n",
    "        left_indices = X[:, feature_idx] <= threshold\n",
    "        right_indices = X[:, feature_idx] > threshold\n",
    "\n",
    "        # Si una de las divisiones está vacía, se devuelve un puntaje infinito\n",
    "        if np.sum(left_indices) == 0 or np.sum(right_indices) == 0:\n",
    "            return float('inf')\n",
    "\n",
    "        # Calcula las verosimilitudes para ambas ramas\n",
    "        left_likelihood = self._calculate_likelihoods(X[left_indices], y[left_indices])\n",
    "        right_likelihood = self._calculate_likelihoods(X[right_indices], y[right_indices])\n",
    "\n",
    "        # El puntaje es la suma de las verosimilitudes negativas\n",
    "        total_score = left_likelihood['score'] + right_likelihood['score']\n",
    "        return total_score\n",
    "\n",
    "    def _calculate_likelihoods(self, X, y):\n",
    "        \"\"\"\n",
    "        Calcula la verosimilitud para cada clase dada una partición de los datos.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Subconjunto de la matriz de características.\n",
    "            y (numpy.ndarray): Subconjunto del vector de etiquetas de clase.\n",
    "\n",
    "        Returns:\n",
    "            dict: Verosimilitudes para cada clase, junto con el puntaje total de la partición.\n",
    "        \"\"\"\n",
    "        classes = np.unique(y)\n",
    "        likelihoods = {'score': 0}  # Inicializa el diccionario con 'score'\n",
    "\n",
    "        for cls in classes:\n",
    "            class_indices = (y == cls)\n",
    "            mu = np.mean(X[class_indices], axis=0)  # Media de la característica\n",
    "            sigma = np.std(X[class_indices], axis=0)  # Desviación estándar\n",
    "\n",
    "            # Evita la división por cero\n",
    "            sigma[sigma == 0] = 1e-4\n",
    "\n",
    "            # Guarda los parámetros de la distribución gaussiana para la clase\n",
    "            likelihoods[cls] = {\n",
    "                'mu': mu,\n",
    "                'sigma': sigma,\n",
    "                'prior': np.mean(class_indices)  # Probabilidad a priori\n",
    "            }\n",
    "\n",
    "            # Calcula el puntaje como la suma negativa del logaritmo de la función de densidad\n",
    "            likelihoods['score'] += -np.sum(np.log(norm.pdf(X[class_indices], mu, sigma) + 1e-6))\n",
    "\n",
    "        return likelihoods\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predice las clases para un conjunto de muestras utilizando el árbol de decisión bayesiano.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Matriz de características de las muestras a predecir.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Vector de predicciones para las muestras.\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            likelihoods = self.model['root']\n",
    "            feature_idx = likelihoods['feature']\n",
    "            threshold = likelihoods['threshold']\n",
    "\n",
    "            # Decide si seguir a la izquierda o a la derecha en el árbol\n",
    "            if x[feature_idx] <= threshold:\n",
    "                probs = self._calculate_posterior(x, likelihoods['left'])\n",
    "            else:\n",
    "                probs = self._calculate_posterior(x, likelihoods['right'])\n",
    "\n",
    "            # Predice la clase con la mayor probabilidad posterior\n",
    "            predictions.append(max(probs, key=probs.get))\n",
    "\n",
    "        return np.array(predictions)\n",
    "\n",
    "    def _calculate_posterior(self, x, likelihoods):\n",
    "        \"\"\"\n",
    "        Calcula las probabilidades posteriores para una muestra dada utilizando las verosimilitudes y las probabilidades a priori.\n",
    "\n",
    "        Args:\n",
    "            x (numpy.ndarray): Muestra individual.\n",
    "            likelihoods (dict): Verosimilitudes y probabilidades a priori calculadas para una rama del árbol.\n",
    "\n",
    "        Returns:\n",
    "            dict: Probabilidades posteriores para cada clase.\n",
    "        \"\"\"\n",
    "        posteriors = {}\n",
    "        for cls, stats in likelihoods.items():\n",
    "            if cls == 'score':  # Ignora 'score' en los cálculos\n",
    "                continue\n",
    "            prior = stats['prior']\n",
    "            # Calcula la probabilidad del valor x dado la distribución gaussiana para la clase\n",
    "            likelihood = np.prod(norm.pdf(x, stats['mu'], stats['sigma']) + 1e-6)\n",
    "            # Multiplica por la probabilidad a priori para obtener la probabilidad posterior\n",
    "            posteriors[cls] = prior * likelihood\n",
    "        return posteriors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar y entrenar el modelo\n",
    "model = BayesianDecisionTree()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predecir en el conjunto de prueba\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluar la precisión del modelo\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
