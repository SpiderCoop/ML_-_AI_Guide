{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo de Árboles de Decisión\n",
    "\n",
    "Los **árboles de decisión** son modelos predictivos utilizados tanto en problemas de clasificación como de regresión. Estos modelos se basan en una estructura de árbol, donde cada nodo representa una característica o atributo del conjunto de datos, y cada rama representa un valor posible de ese atributo. Las hojas del árbol representan las predicciones finales o la salida del modelo.\n",
    "\n",
    "## Estructura de un Árbol de Decisión\n",
    "\n",
    "Un árbol de decisión está compuesto por tres elementos clave:\n",
    "\n",
    "1. **Nodos de Decisión (nodos internos):** Representan una prueba o condición sobre un atributo. \n",
    "2. **Ramas:** Conectan los nodos y representan el resultado de una prueba o condición.\n",
    "3. **Nodos Hoja (nodos terminales):** Representan la decisión o predicción final, que puede ser una clase (en clasificación) o un valor continuo (en regresión).\n",
    "\n",
    "El árbol se construye mediante un proceso de particionamiento recursivo. Comienza en la raíz y se realizan divisiones sucesivas de los datos en subconjuntos más pequeños, utilizando el atributo que mejor separa los datos según un criterio de división. Este proceso continúa hasta que se cumple un criterio de parada, como la profundidad máxima del árbol o un número mínimo de muestras en un nodo.\n",
    "\n",
    "### Criterios de División\n",
    "\n",
    "Los criterios de división determinan cómo se eligen los atributos en cada nodo para dividir los datos:\n",
    "\n",
    "1. **Índice de Gini:** Mide la pureza de los nodos. Un nodo es puro si contiene solo muestras de una clase. El índice de Gini se utiliza comúnmente en clasificación.\n",
    "   \n",
    "   $\n",
    "   Gini = 1 - \\sum_{i=1}^{n} p_i^2\n",
    "   $\n",
    "   donde $p_i$ es la proporción de elementos en la clase $i$.\n",
    "\n",
    "2. **Entropía:** Mide la incertidumbre o impureza en un nodo. Es utilizada en la construcción de árboles de decisión como el algoritmo ID3.\n",
    "   \n",
    "   $\n",
    "   Entropía = - \\sum_{i=1}^{n} p_i \\log_2(p_i)\n",
    "   $\n",
    "    donde $p_i$ es la proporción de elementos en la clase $i$.\n",
    "\n",
    "3. **Reducción de Varianza (para regresión):** En problemas de regresión, se utiliza la varianza de las muestras en los nodos para decidir las divisiones. Se elige la partición que minimiza la varianza dentro de los nodos hijos.\n",
    "\n",
    "### Ecuación del Modelo\n",
    "\n",
    "La predicción para un árbol de decisión en un problema de clasificación se puede representar como:\n",
    "\n",
    "$\n",
    "\\hat{y} = \\text{Clase más frecuente en la hoja correspondiente a los valores de } x_1, x_2, \\ldots, x_n\n",
    "$\n",
    "\n",
    "Para un problema de regresión, la predicción se obtiene promediando los valores de las muestras en la hoja:\n",
    "\n",
    "$\n",
    "\\hat{y} = \\frac{1}{m} \\sum_{i=1}^{m} y_i\n",
    "$\n",
    "donde $m$ es el número de muestras en la hoja, y $y_i$ es el valor de la variable dependiente.\n",
    "\n",
    "## Ventajas y Desventajas\n",
    "\n",
    "- **Ventajas:**\n",
    "- **Interpretabilidad:** Los árboles de decisión son fáciles de entender y visualizar.\n",
    "- **No linealidad:** Capturan relaciones no lineales entre las variables independientes y la variable dependiente.\n",
    "- **No requieren normalización:** No es necesario escalar las características, y pueden manejar variables categóricas y continuas.\n",
    "\n",
    "- **Desventajas:**\n",
    "- **Sobreajuste:** Los árboles de decisión pueden sobreajustarse fácilmente a los datos de entrenamiento, lo que reduce su capacidad de generalización.\n",
    "- **Inestabilidad:** Pequeñas variaciones en los datos pueden llevar a cambios significativos en la estructura del árbol.\n",
    "\n",
    "## Ejemplo de Aplicación\n",
    "\n",
    "Supongamos que queremos predecir si un cliente comprará un producto en función de sus características demográficas (edad, ingreso, etc.).\n",
    "\n",
    "- **Datos de ejemplo:**\n",
    "  \n",
    "  | Edad | Ingreso | Compra |\n",
    "  |------|---------|--------|\n",
    "  | 25   | Bajo    | No     |\n",
    "  | 35   | Alto    | Sí     |\n",
    "  | 45   | Medio   | Sí     |\n",
    "  | 50   | Alto    | No     |\n",
    "\n",
    "- **Construcción del Árbol:**\n",
    "  Utilizando un algoritmo como CART (Classification and Regression Tree), el árbol se construye seleccionando la característica (Edad o Ingreso) que mejor divide los datos según un criterio como el índice de Gini o la entropía. El proceso continúa hasta que se cumplen las condiciones de parada.\n",
    "\n",
    "- **Predicción:**\n",
    "  Para un nuevo cliente de 40 años con ingreso medio, el árbol de decisión determinará si comprará o no en función de las reglas aprendidas en los nodos.\n",
    "\n",
    "### Implementación en Python\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicción para el nuevo cliente: Sí\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Datos de ejemplo\n",
    "X = np.array([\n",
    "    [25, 'Bajo'], \n",
    "    [35, 'Alto'], \n",
    "    [45, 'Medio'], \n",
    "    [50, 'Alto']\n",
    "])\n",
    "y = np.array(['No', 'Sí', 'Sí', 'No'])\n",
    "\n",
    "# Convertir la columna categórica ('Bajo', 'Alto', 'Medio') en valores numéricos\n",
    "le = LabelEncoder()\n",
    "X[:, 1] = le.fit_transform(X[:, 1])\n",
    "\n",
    "# Convertir X de nuevo a tipo numérico (entero)\n",
    "X = X.astype(float)\n",
    "\n",
    "# Crear y entrenar el modelo\n",
    "model = DecisionTreeClassifier(criterion='gini', max_depth=3)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predicción para un nuevo cliente\n",
    "nuevo_cliente = np.array([[40, 'Medio']])\n",
    "\n",
    "# Convertir la característica categórica del nuevo cliente\n",
    "nuevo_cliente[:, 1] = le.transform(nuevo_cliente[:, 1])\n",
    "\n",
    "# Convertir nuevo_cliente a tipo numérico (entero)\n",
    "nuevo_cliente = nuevo_cliente.astype(float)\n",
    "\n",
    "# Realizar la predicción\n",
    "prediccion = model.predict(nuevo_cliente)\n",
    "\n",
    "print(f'Predicción para el nuevo cliente: {prediccion[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arboles de decision Bayesianos\n",
    "\n",
    "El **Árbol de Decisión Bayesiano** es una extensión de los árboles de decisión tradicionales que incorpora principios de la estadística bayesiana para tomar decisiones en cada nodo. A diferencia de los árboles de decisión estándar, donde las decisiones se basan en divisiones determinísticas según una métrica como la ganancia de información o el índice de Gini, un árbol de decisión bayesiano asigna distribuciones de probabilidad a las características y calcula probabilidades condicionadas en cada nodo.\n",
    "\n",
    "## Estructura del Modelo\n",
    "\n",
    "1. **Nodos Internos**: En un árbol de decisión bayesiano, cada nodo interno representa una prueba o condición basada en una o más características del conjunto de datos. En lugar de realizar una división basada en un solo valor, se asigna una distribución de probabilidad a la característica relevante.\n",
    "\n",
    "2. **Distribución de Probabilidad**: Cada característica en un nodo puede tener una distribución de probabilidad asociada. Por ejemplo, si una característica es continua, podría estar modelada con una distribución normal (Gaussiana). Si es categórica, podría estar modelada con una distribución multinomial.\n",
    "\n",
    "3. **Hiperparámetros**: Los hiperparámetros son los parámetros que definen la distribución de probabilidad. En el caso de una distribución normal, los hiperparámetros serían la media y la varianza. Estos hiperparámetros pueden ser ajustados durante el entrenamiento del modelo.\n",
    "\n",
    "4. **Probabilidades Condicionadas**: En cada nodo, se calculan las probabilidades condicionadas de las diferentes clases, dado el valor de la característica. Estas probabilidades se actualizan utilizando el teorema de Bayes a medida que se recorre el árbol.\n",
    "\n",
    "5. **Clasificación Final**: Para clasificar una instancia, se recorre el árbol desde la raíz hasta una hoja, multiplicando las probabilidades a lo largo del camino. La clase con la mayor probabilidad final es asignada a la instancia.\n",
    "\n",
    "### Ejemplo Matemático\n",
    "\n",
    "Consideremos un ejemplo simple con una característica continua $x$ y dos posibles clases $C_1$ y $C_2$.\n",
    "\n",
    "1. **Distribución de Probabilidad**: Supongamos que $x$ sigue una distribución normal:\n",
    "   $\n",
    "   P(x|C_i) = \\frac{1}{\\sqrt{2\\pi \\sigma_i^2}} \\exp\\left(-\\frac{(x - \\mu_i)^2}{2\\sigma_i^2}\\right)\n",
    "   $\n",
    "   donde $\\mu_i$ y $\\sigma_i^2$ son los hiperparámetros (media y varianza) para la clase $C_i$.\n",
    "\n",
    "2. **Probabilidad a Priori**: Se asignan probabilidades a priori a las clases, $P(C_1)$ y $P(C_2)$.\n",
    "\n",
    "3. **Probabilidad Posteriori**: Al observar un valor $x$, se calculan las probabilidades posteriores utilizando el teorema de Bayes:\n",
    "   $\n",
    "   P(C_i|x) = \\frac{P(x|C_i) P(C_i)}{P(x)}\n",
    "   $\n",
    "   donde $P(x) = \\sum_{i} P(x|C_i)P(C_i)$ es la probabilidad marginal de $x$.\n",
    "\n",
    "4. **Decisión**: Se asigna la clase con la mayor probabilidad posterior:\n",
    "   $\n",
    "   \\hat{C} = \\arg\\max_{i} P(C_i|x)\n",
    "   $\n",
    "\n",
    "## Ventajas y Desventajas\n",
    "\n",
    "- **Ventajas**:\n",
    "  - Permite incorporar incertidumbre en las decisiones.\n",
    "  - Es más robusto ante la variabilidad en los datos.\n",
    "  - Puede manejar datos con ruido y datos faltantes de manera más efectiva.\n",
    "\n",
    "- **Desventajas**:\n",
    "  - Mayor complejidad computacional debido al cálculo de probabilidades en cada nodo.\n",
    "  - Requiere estimar y ajustar correctamente las distribuciones de probabilidad y sus hiperparámetros.\n",
    "\n",
    "### Aplicaciones\n",
    "\n",
    "Este modelo es útil en situaciones donde es importante capturar la incertidumbre en la toma de decisiones, como en sistemas de diagnóstico médico, detección de fraudes, y otras aplicaciones donde los riesgos asociados con decisiones incorrectas son altos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import norm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Simular datos\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "\n",
    "# Característica 1: Peso\n",
    "X1_class0 = np.random.normal(160, 10, n_samples // 2)\n",
    "X1_class1 = np.random.normal(180, 10, n_samples // 2)\n",
    "\n",
    "# Característica 2: Tamaño\n",
    "X2_class0 = np.random.normal(5, 1, n_samples // 2)\n",
    "X2_class1 = np.random.normal(7, 1, n_samples // 2)\n",
    "\n",
    "# Etiquetas\n",
    "y_class0 = np.zeros(n_samples // 2)\n",
    "y_class1 = np.ones(n_samples // 2)\n",
    "\n",
    "# Unir datos\n",
    "X = np.vstack((np.hstack((X1_class0, X1_class1)), np.hstack((X2_class0, X2_class1)))).T\n",
    "y = np.hstack((y_class0, y_class1))\n",
    "\n",
    "# Crear DataFrame\n",
    "df = pd.DataFrame(X, columns=['Peso', 'Tamaño'])\n",
    "df['Clase'] = y\n",
    "\n",
    "# Dividir en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianDecisionTree:\n",
    "    \"\"\"\n",
    "    Un árbol de decisión que utiliza un enfoque bayesiano para la clasificación.\n",
    "    Este modelo combina la estructura de un árbol de decisión con la estimación\n",
    "    de probabilidades basadas en distribuciones Gaussianas.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Inicializa el modelo como un diccionario vacío que contendrá la estructura del árbol.\"\"\"\n",
    "        self.model = {}\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Ajusta el modelo a los datos de entrenamiento.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Matriz de características de forma (n_samples, n_features).\n",
    "            y (numpy.ndarray): Vector de etiquetas de clase de forma (n_samples,).\n",
    "\n",
    "        \"\"\"\n",
    "        # La raíz del árbol se construye dividiendo los datos en nodos\n",
    "        self.model['root'] = self._split_node(X, y)\n",
    "\n",
    "    def _split_node(self, X, y):\n",
    "        \"\"\"\n",
    "        Encuentra la mejor característica y umbral para dividir los datos en un nodo.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Matriz de características.\n",
    "            y (numpy.ndarray): Vector de etiquetas de clase.\n",
    "\n",
    "        Returns:\n",
    "            dict: Nodo dividido con la característica seleccionada, umbral y\n",
    "                  las distribuciones de probabilidad condicional para cada rama (izquierda y derecha).\n",
    "        \"\"\"\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "        best_score = float('inf')  # Inicializa el mejor puntaje con infinito\n",
    "\n",
    "        # Itera sobre todas las características para encontrar la mejor\n",
    "        for feature_idx in range(X.shape[1]):\n",
    "            thresholds = np.unique(X[:, feature_idx])  # Umbrales únicos de la característica\n",
    "            for threshold in thresholds:\n",
    "                score = self._calculate_score(X, y, feature_idx, threshold)\n",
    "                if score < best_score:  # Busca minimizar el puntaje\n",
    "                    best_score = score\n",
    "                    best_feature = feature_idx\n",
    "                    best_threshold = threshold\n",
    "\n",
    "        # Divide los datos en dos subconjuntos: izquierdo y derecho\n",
    "        left_indices = X[:, best_feature] <= best_threshold\n",
    "        right_indices = X[:, best_feature] > best_threshold\n",
    "\n",
    "        # Crea el nodo con la mejor característica y umbral encontrados\n",
    "        left_node = {\n",
    "            'feature': best_feature,\n",
    "            'threshold': best_threshold,\n",
    "            'left': self._calculate_likelihoods(X[left_indices], y[left_indices]),\n",
    "            'right': self._calculate_likelihoods(X[right_indices], y[right_indices])\n",
    "        }\n",
    "\n",
    "        return left_node\n",
    "\n",
    "    def _calculate_score(self, X, y, feature_idx, threshold):\n",
    "        \"\"\"\n",
    "        Calcula un puntaje para una división específica basada en la suma de las probabilidades logarítmicas negativas.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Matriz de características.\n",
    "            y (numpy.ndarray): Vector de etiquetas de clase.\n",
    "            feature_idx (int): Índice de la característica seleccionada.\n",
    "            threshold (float): Umbral de la característica para la división.\n",
    "\n",
    "        Returns:\n",
    "            float: Puntaje basado en la suma de las probabilidades logarítmicas negativas.\n",
    "        \"\"\"\n",
    "        # Divide los datos según el umbral\n",
    "        left_indices = X[:, feature_idx] <= threshold\n",
    "        right_indices = X[:, feature_idx] > threshold\n",
    "\n",
    "        # Si una de las divisiones está vacía, se devuelve un puntaje infinito\n",
    "        if np.sum(left_indices) == 0 or np.sum(right_indices) == 0:\n",
    "            return float('inf')\n",
    "\n",
    "        # Calcula las verosimilitudes para ambas ramas\n",
    "        left_likelihood = self._calculate_likelihoods(X[left_indices], y[left_indices])\n",
    "        right_likelihood = self._calculate_likelihoods(X[right_indices], y[right_indices])\n",
    "\n",
    "        # El puntaje es la suma de las verosimilitudes negativas\n",
    "        total_score = left_likelihood['score'] + right_likelihood['score']\n",
    "        return total_score\n",
    "\n",
    "    def _calculate_likelihoods(self, X, y):\n",
    "        \"\"\"\n",
    "        Calcula la verosimilitud para cada clase dada una partición de los datos.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Subconjunto de la matriz de características.\n",
    "            y (numpy.ndarray): Subconjunto del vector de etiquetas de clase.\n",
    "\n",
    "        Returns:\n",
    "            dict: Verosimilitudes para cada clase, junto con el puntaje total de la partición.\n",
    "        \"\"\"\n",
    "        classes = np.unique(y)\n",
    "        likelihoods = {'score': 0}  # Inicializa el diccionario con 'score'\n",
    "\n",
    "        for cls in classes:\n",
    "            class_indices = (y == cls)\n",
    "            mu = np.mean(X[class_indices], axis=0)  # Media de la característica\n",
    "            sigma = np.std(X[class_indices], axis=0)  # Desviación estándar\n",
    "\n",
    "            # Evita la división por cero\n",
    "            sigma[sigma == 0] = 1e-4\n",
    "\n",
    "            # Guarda los parámetros de la distribución gaussiana para la clase\n",
    "            likelihoods[cls] = {\n",
    "                'mu': mu,\n",
    "                'sigma': sigma,\n",
    "                'prior': np.mean(class_indices)  # Probabilidad a priori\n",
    "            }\n",
    "\n",
    "            # Calcula el puntaje como la suma negativa del logaritmo de la función de densidad\n",
    "            likelihoods['score'] += -np.sum(np.log(norm.pdf(X[class_indices], mu, sigma) + 1e-6))\n",
    "\n",
    "        return likelihoods\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predice las clases para un conjunto de muestras utilizando el árbol de decisión bayesiano.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Matriz de características de las muestras a predecir.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Vector de predicciones para las muestras.\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            likelihoods = self.model['root']\n",
    "            feature_idx = likelihoods['feature']\n",
    "            threshold = likelihoods['threshold']\n",
    "\n",
    "            # Decide si seguir a la izquierda o a la derecha en el árbol\n",
    "            if x[feature_idx] <= threshold:\n",
    "                probs = self._calculate_posterior(x, likelihoods['left'])\n",
    "            else:\n",
    "                probs = self._calculate_posterior(x, likelihoods['right'])\n",
    "\n",
    "            # Predice la clase con la mayor probabilidad posterior\n",
    "            predictions.append(max(probs, key=probs.get))\n",
    "\n",
    "        return np.array(predictions)\n",
    "\n",
    "    def _calculate_posterior(self, x, likelihoods):\n",
    "        \"\"\"\n",
    "        Calcula las probabilidades posteriores para una muestra dada utilizando las verosimilitudes y las probabilidades a priori.\n",
    "\n",
    "        Args:\n",
    "            x (numpy.ndarray): Muestra individual.\n",
    "            likelihoods (dict): Verosimilitudes y probabilidades a priori calculadas para una rama del árbol.\n",
    "\n",
    "        Returns:\n",
    "            dict: Probabilidades posteriores para cada clase.\n",
    "        \"\"\"\n",
    "        posteriors = {}\n",
    "        for cls, stats in likelihoods.items():\n",
    "            if cls == 'score':  # Ignora 'score' en los cálculos\n",
    "                continue\n",
    "            prior = stats['prior']\n",
    "            # Calcula la probabilidad del valor x dado la distribución gaussiana para la clase\n",
    "            likelihood = np.prod(norm.pdf(x, stats['mu'], stats['sigma']) + 1e-6)\n",
    "            # Multiplica por la probabilidad a priori para obtener la probabilidad posterior\n",
    "            posteriors[cls] = prior * likelihood\n",
    "        return posteriors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Inicializar y entrenar el modelo\n",
    "model = BayesianDecisionTree()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predecir en el conjunto de prueba\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluar la precisión del modelo\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
