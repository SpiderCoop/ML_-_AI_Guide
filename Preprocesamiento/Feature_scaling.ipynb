{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "\n",
    "Feature scaling, o escalado de características, es una técnica utilizada en machine learning para estandarizar el rango de las características (features) de los datos. Es crucial cuando se trabaja con algoritmos que son sensibles a las magnitudes de las características, como el gradiente descendente, SVM (Máquinas de Vectores de Soporte) y KNN (K-Nearest Neighbors), entre otros.\n",
    "\n",
    "Normalmente, es suficiente aplicar uno de los métodos de feature scaling a todo el dataset.Sin embargo, si se tiene una razón clara, se puede escalar diferentes subconjuntos de características con distintos métodos, pero siempre asegurando de que esto tenga sentido para el modelo y los datos.\n",
    "\n",
    "Es importante que este proceso se realice despues de hacer la separacion de la base de datos en el conjunto de entrenamiento y el de validacion. Si no se hace asi, estariamos tomadno informacion del conjunto de validacion en el entrenamiento del modelo, haciendo que la validacion no refleje el desempeño del modelo ante nuevos datos.\n",
    "\n",
    "Otro punto importante es que una vez que se haga el escalado, se guarden los parametros para realizar la misma regla en nuevos datos, es decir que, si por ejemplo usamos estandarizacion, la media y varianza que calculemos con el conjunto de entrenamiento son las mismas con las que vamos a escalar los datos de validacion y nuevos datos, no se calculan de nuevo. Esto permite tener consistencia entre los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Normalización (Min-Max Scaling)\n",
    "\n",
    "La normalización ajusta los valores de las características para que caigan dentro de un rango específico, generalmente [0, 1]. La fórmula es:\n",
    "\n",
    "$ X_{\\text{norm}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}} $\n",
    "\n",
    "Cuándo usarla:\n",
    "\n",
    "- Datos con límites conocidos: Si sabes que tus datos siempre estarán dentro de un rango específico, la normalización es apropiada.\n",
    "- Algoritmos basados en distancia: Como K-Nearest Neighbors (KNN), K-Means y algoritmos de clustering, que se ven afectados por las magnitudes de las características.\n",
    "- Redes neuronales: Cuando utilizas redes neuronales, la normalización puede ayudar a que la red aprenda más rápido y de manera más estable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos Normalizados:\n",
      " [[0.   0.  ]\n",
      " [0.25 0.25]\n",
      " [0.5  0.5 ]\n",
      " [1.   1.  ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Datos de ejemplo\n",
    "data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]\n",
    "\n",
    "# Normalización\n",
    "scaler = MinMaxScaler()\n",
    "data_normalized = scaler.fit_transform(data)\n",
    "print(\"Datos Normalizados:\\n\", data_normalized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Estandarización (Z-score Scaling)\n",
    "\n",
    "La estandarización ajusta los valores de las características para que tengan una media de 0 y una desviación estándar de 1. La fórmula es:\n",
    "\n",
    "$ X_{\\text{std}} = \\frac{X - \\mu}{\\sigma} $\n",
    "\n",
    "donde $\\mu$ es la media y $\\sigma$ es la desviación estándar de la característica.\n",
    "\n",
    "Cuándo usarla:\n",
    "\n",
    "- Algoritmos basados en estadísticas: Como regresión lineal, regresión logística y análisis discriminante lineal (LDA), que asumen que los datos están distribuidos normalmente.\n",
    "- PCA y otros métodos de reducción de dimensionalidad: Estos métodos son sensibles a la escala de las características.\n",
    "- Datos con diferentes unidades: Si tus datos tienen diferentes unidades de medida, la estandarización puede hacer que las características sean comparables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos Estandarizados:\n",
      " [[-1.18321596 -1.18321596]\n",
      " [-0.50709255 -0.50709255]\n",
      " [ 0.16903085  0.16903085]\n",
      " [ 1.52127766  1.52127766]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Datos de ejemplo\n",
    "data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]\n",
    "\n",
    "# Estandarización\n",
    "scaler = StandardScaler()\n",
    "data_standardized = scaler.fit_transform(data)\n",
    "print(\"Datos Estandarizados:\\n\", data_standardized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Robust Scaling\n",
    "\n",
    "El escalado robusto utiliza la mediana y los cuartiles en lugar de la media y la desviación estándar, lo que lo hace menos sensible a los outliers.\n",
    "\n",
    "$X_{\\text{robust}} = \\frac{X - \\text{mediana}}{\\text{IQR}}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos con escalado robusto:\n",
      " [[-0.85714286 -0.85714286]\n",
      " [-0.28571429 -0.28571429]\n",
      " [ 0.28571429  0.28571429]\n",
      " [ 1.42857143  1.42857143]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Datos de ejemplo\n",
    "data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]\n",
    "\n",
    "# Escalado robusto\n",
    "scaler = RobustScaler()\n",
    "data_robust = scaler.fit_transform(data)\n",
    "print(\"Datos con escalado robusto:\\n\", data_robust)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. MaxAbs Scaling\n",
    "\n",
    "El escalado MaxAbs ajusta los valores de las características dividiendo por el valor absoluto máximo, manteniendo los datos dentro del rango [-1, 1]. Es útil para datos que ya están centrados alrededor de 0 y se desea mantener la dispersión.\n",
    "\n",
    "$X_{\\text{maxabs}} = \\frac{X}{\\left| X_{\\text{max}} \\right|}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos con escalado MaxAbs:\n",
      " [[-1.          0.11111111]\n",
      " [-0.5         0.33333333]\n",
      " [ 0.          0.55555556]\n",
      " [ 1.          1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "# Datos de ejemplo\n",
    "data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]\n",
    "\n",
    "# Escalado MaxAbs\n",
    "scaler = MaxAbsScaler()\n",
    "data_maxabs = scaler.fit_transform(data)\n",
    "print(\"Datos con escalado MaxAbs:\\n\", data_maxabs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Escalado logarítmico (Log Scaling)\n",
    "El escalado logarítmico se utiliza para transformar características que tienen una distribución altamente sesgada. Se aplica la función logarítmica para reducir la asimetría.\n",
    "\n",
    "$X_{log} = \\log{(X+1)}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos con escalado logarítmico:\n",
      " [[0.69314718 2.39789527]\n",
      " [1.09861229 4.61512052]\n",
      " [1.38629436 6.90875478]\n",
      " [1.60943791 9.21044037]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Datos de ejemplo\n",
    "data = np.array([[1, 10], [2, 100], [3, 1000], [4, 10000]])\n",
    "\n",
    "# Escalado logarítmico\n",
    "data_log = np.log1p(data)\n",
    "print(\"Datos con escalado logarítmico:\\n\", data_log)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Escalado de raíz cuadrada (Square Root Scaling)\n",
    "El escalado de raíz cuadrada es útil para transformar características que tienen una distribución sesgada, similar al escalado logarítmico, pero menos severo.\n",
    "\n",
    "$X_{sqrt} = \\sqrt{X}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos de ejemplo\n",
    "data = np.array([[1, 10], [4, 100], [9, 1000], [16, 10000]])\n",
    "\n",
    "# Escalado de raíz cuadrada\n",
    "data_sqrt = np.sqrt(data)\n",
    "print(\"Datos con escalado de raíz cuadrada:\\n\", data_sqrt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Escalado por cuantiles (Quantile Transformation)\n",
    "La transformación por cuantiles convierte las características para seguir una distribución deseada Este método es útil cuando quieres que las características se ajusten a una distribución específica, lo que puede ser beneficioso para algunos algoritmos de machine learning que son sensibles a la forma de la distribución de los datos.\n",
    "\n",
    "Método:\n",
    "\n",
    "1. Ordena los datos: Los datos se ordenan en función de sus valores.\n",
    "2. Asignar cuantiles: Cada dato se mapea a su cuantil correspondiente.\n",
    "3. Transformar: Se transforman los datos en función de la distribución deseada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "# Datos de ejemplo\n",
    "data = [[1, 2], [2, 3], [3, 4], [4, 5]]\n",
    "\n",
    "# Transformación por cuantiles\n",
    "scaler = QuantileTransformer(output_distribution='normal')\n",
    "data_quantile = scaler.fit_transform(data)\n",
    "print(\"Datos con transformación por cuantiles:\\n\", data_quantile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 8. Normalización L2 (L2 Normalization)\n",
    "La normalización L2, también conocida como normalización de vectores, ajusta las características para que la norma L2 de cada vector sea igual a 1. La norma L2 de un vector es la raíz cuadrada de la suma de los cuadrados de sus componentes.\n",
    "\n",
    "Formula:\n",
    "\n",
    "Para un vector $[x_1,x_2,...,x_n]$, la normalización L2 se calcula como:\n",
    "\n",
    "$\\text{Norma L2} = \\sqrt{x_1^2 + x_2^2 + \\cdots + x_n^2}$\n",
    "\n",
    "La normalización L2 ajusta cada componente del vector a:\n",
    "\n",
    "$x' = \\frac{x}{\\text{Norma L2}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# Datos de ejemplo\n",
    "data = [[4, 1], [2, 2], [1, 3]]\n",
    "\n",
    "# Normalización L2\n",
    "scaler = Normalizer(norm='l2')\n",
    "data_l2 = scaler.fit_transform(data)\n",
    "print(\"Datos con normalización L2:\\n\", data_l2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
