{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularización\n",
    "\n",
    "La **regularización** es una técnica utilizada en los modelos de machine learning para evitar el **sobreajuste** (overfitting), que ocurre cuando un modelo se ajusta demasiado a los datos de entrenamiento y, por lo tanto, tiene un rendimiento deficiente en datos nuevos o no vistos. El objetivo principal de la regularización es encontrar un equilibrio entre el ajuste del modelo a los datos de entrenamiento y su capacidad de generalización.\n",
    "\n",
    "Cuando un modelo es demasiado complejo, como puede ser el caso con muchos parámetros o características, corre el riesgo de memorizar los datos en lugar de aprender patrones subyacentes. Esto puede llevar a que el modelo tenga un bajo error en los datos de entrenamiento, pero un alto error en datos de prueba o de validación.\n",
    "\n",
    "La regularización aborda este problema al agregar un **término de penalización** a la función de costo o pérdida del modelo, que restringe o reduce el tamaño de los coeficientes del modelo, lo que impide que el modelo se ajuste demasiado a los datos.\n",
    "\n",
    "## Tipos de Regularización\n",
    "\n",
    "Existen varios tipos de regularización, pero los dos más comunes son **L2** (Ridge) y **L1** (Lasso):\n",
    "\n",
    "1. **Regularización L2 (Ridge)**:\n",
    "   En la regularización L2, se añade una penalización proporcional al cuadrado de la magnitud de los coeficientes del modelo. La función de costo modificada se define como:\n",
    "\n",
    "   $$\n",
    "   J(\\theta) = \\text{L}(\\theta) + \\lambda \\sum_{j=1}^p \\theta_j^2\n",
    "   $$\n",
    "\n",
    "   Donde:\n",
    "   - $\\text{L}(\\theta)$ es la función de pérdida (por ejemplo, error cuadrático medio).\n",
    "   - $\\lambda$ es el parámetro de regularización que controla la fuerza de la penalización.\n",
    "   - $\\theta_j$ son los coeficientes del modelo.\n",
    "   \n",
    "   Esto favorece coeficientes más pequeños, pero no necesariamente cero, por lo que tiende a reducir la varianza del modelo sin eliminar características por completo.\n",
    "\n",
    "2. **Regularización L1 (Lasso)**:\n",
    "   En la regularización L1, la penalización es proporcional a la magnitud absoluta de los coeficientes. La función de costo se define como:\n",
    "\n",
    "   $$\n",
    "   J(\\theta) = \\text{L}(\\theta) + \\lambda \\sum_{j=1}^p |\\theta_j|\n",
    "   $$\n",
    "\n",
    "   Este tipo de regularización tiende a reducir algunos coeficientes a cero, lo que efectivamente elimina características irrelevantes. Esto también la convierte en una técnica útil para la **selección de características**.\n",
    "\n",
    "3. **Elastic Net**\n",
    "    Elastic Net es una combinación de L1 y L2, que incluye ambos términos de regularización:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\text{L}(\\theta) + \\lambda_1 \\sum_{j=1}^p |\\theta_j| + \\lambda_2 \\sum_{j=1}^p \\theta_j^2\n",
    "$$\n",
    "\n",
    "## Efecto de la Regularización\n",
    "\n",
    "- **Evita el sobreajuste**: Al penalizar grandes coeficientes, la regularización fuerza al modelo a ser más simple y menos propenso a aprender ruido o variabilidad aleatoria en los datos de entrenamiento.\n",
    "- **Mejora la generalización**: Ayuda al modelo a ser más robusto frente a datos no vistos, mejorando su capacidad de generalizar.\n",
    "- **Control de complejidad**: El parámetro $\\lambda$ controla la cantidad de regularización. Un $\\lambda$ grande puede hacer que el modelo sea demasiado simple, mientras que un $\\lambda$ pequeño puede hacer que el modelo se ajuste demasiado a los datos de entrenamiento.\n",
    "\n",
    "## Elección de $\\lambda$\n",
    "\n",
    "El valor de $\\lambda$ se elige a través de técnicas como la **validación cruzada**, donde el modelo se entrena varias veces con diferentes valores de $\\lambda$, y se selecciona el que ofrece el mejor rendimiento en datos de validación.\n",
    "\n",
    "## Aplicaciones de la Regularización\n",
    "\n",
    "- **Regresión lineal y logística**: Regularización L1 y L2 se utilizan comúnmente para mejorar estos modelos, especialmente cuando el número de características es grande.\n",
    "- **Redes neuronales**: En redes neuronales, técnicas como **dropout** también se utilizan como un tipo de regularización para evitar el sobreajuste.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exactitud del modelo: 1.0\n",
      "\n",
      "Reporte de clasificación:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         1\n",
      "           1       1.00      1.00      1.00         3\n",
      "\n",
      "    accuracy                           1.00         4\n",
      "   macro avg       1.00      1.00      1.00         4\n",
      "weighted avg       1.00      1.00      1.00         4\n",
      "\n",
      "\n",
      "Coeficientes del modelo: [[ 0.83259795 -1.04303454]]\n",
      "Intercepto: [0.23036366]\n"
     ]
    }
   ],
   "source": [
    "# Importar las librerías necesarias\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Crear un conjunto de datos de ejemplo\n",
    "# Variables de entrada X (características)\n",
    "X = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6], [6, 7],\n",
    "              [1, 0], [2, 1], [3, 2], [4, 3], [5, 4], [6, 5]])\n",
    "\n",
    "# Etiquetas de salida y (0 o 1)\n",
    "y = np.array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1])\n",
    "\n",
    "# Dividir el conjunto de datos en conjunto de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Crear el modelo de regresión logística regularizada (Ridge)\n",
    "# El parámetro 'C' es el inverso de la regularización. Cuanto menor es, más fuerte es la regularización.\n",
    "model = LogisticRegression(penalty='l2', C=1.0, solver='liblinear')\n",
    "\n",
    "# Entrenar el modelo con los datos de entrenamiento\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Realizar predicciones con los datos de prueba\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluar el desempeño del modelo\n",
    "print(\"Exactitud del modelo:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nReporte de clasificación:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Mostrar los coeficientes aprendidos por el modelo\n",
    "print(\"\\nCoeficientes del modelo:\", model.coef_)\n",
    "print(\"Intercepto:\", model.intercept_)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
