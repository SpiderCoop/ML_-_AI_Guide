{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Regression (SVR)\n",
    "\n",
    "Support Vector Regression (SVR) es una extensión del modelo Support Vector Machine (SVM), que se utiliza principalmente para problemas de regresión. Al igual que SVM, SVR intenta encontrar un hiperplano óptimo que separe los datos, pero en lugar de clasificación, busca predecir un valor continuo. La idea principal es encontrar una función que tenga un máximo desvío permitido $\\epsilon$ del valor real para todos los puntos de entrenamiento, al mismo tiempo que es tan plana como sea posible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesamiento Previo de los Datos\n",
    "\n",
    "Para que SVR funcione de manera eficiente, a menudo es necesario realizar algunos pasos de preprocesamiento:\n",
    "\n",
    "- **Estandarización**: Es importante estandarizar o normalizar los datos, ya que SVR se basa en la distancia entre puntos de datos y la elección de los parámetros del kernel (como el kernel de RBF) es sensible a la escala de los datos.\n",
    "- **Manejo de valores nulos**: SVR no puede manejar valores nulos directamente, por lo que cualquier dato faltante debe ser imputado o eliminado.\n",
    "- **Codificación de variables categóricas**: Si existen variables categóricas, deben ser convertidas a formato numérico, generalmente usando codificación one-hot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estructura y Ecuación Matemática del Modelo\n",
    "\n",
    "El objetivo del SVR es encontrar una función $f(x)$ que prediga el valor de salida $y$ de la siguiente manera:\n",
    "\n",
    "$$ f(x) = \\langle w, x \\rangle + b $$\n",
    "\n",
    "Donde:\n",
    "- $\\langle w, x \\rangle$ es el producto punto entre el vector de pesos $w$ y el vector de características $x$.\n",
    "- $b$ es el término de sesgo o bias.\n",
    "\n",
    "En SVR, se minimiza la función de costo bajo la restricción de que los errores de predicción se encuentren dentro de un margen de tolerancia $\\epsilon$:\n",
    "\n",
    "$$\n",
    "\\text{Minimizar: } \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^{n} (\\xi_i + \\xi_i^*)\n",
    "$$\n",
    "\n",
    "Sujeto a las restricciones:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y_i - \\langle w, x_i \\rangle - b &\\leq \\epsilon + \\xi_i \\\\\n",
    "\\langle w, x_i \\rangle + b - y_i &\\leq \\epsilon + \\xi_i^* \\\\\n",
    "\\xi_i, \\xi_i^* &\\geq 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "- $\\xi_i$ y $\\xi_i^*$ son variables de holgura que permiten penalizar las predicciones que se encuentran fuera del margen $\\epsilon$.\n",
    "- $C$ es un parámetro de regularización que controla la compensación entre el margen de tolerancia y el error permitido en el entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métodos de Estimación de Parámetros o Entrenamiento\n",
    "\n",
    "El entrenamiento del modelo SVR consiste en resolver un problema de optimización cuadrática (QP) con restricciones. Este problema de optimización es generalmente resuelto mediante técnicas de programación cuadrática o métodos como el algoritmo de gradiente.\n",
    "\n",
    "Los pasos principales incluyen:\n",
    "\n",
    "1. **Seleccionar un kernel**: SVR puede utilizar diferentes tipos de kernels (lineal, polinómico, RBF, etc.) para transformar los datos a un espacio de mayor dimensión donde sea más fácil encontrar un hiperplano que divida los datos.\n",
    "2. **Optimización de la función objetivo**: Encontrar los valores de $w$ y $b$ que minimicen la función de costo sujeta a las restricciones mencionadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitaciones, Ventajas y Desventajas\n",
    "\n",
    "**Ventajas:**\n",
    "- Funciona bien con conjuntos de datos pequeños y medianos.\n",
    "- Es efectivo en espacios de alta dimensionalidad y cuando el número de dimensiones es mayor que el número de muestras.\n",
    "- Flexible con la elección de diferentes funciones de kernel.\n",
    "\n",
    "**Desventajas:**\n",
    "- No es escalable para grandes conjuntos de datos, ya que el tiempo de entrenamiento aumenta significativamente con el número de muestras.\n",
    "- La selección de los hiperparámetros (como $C$ y $\\epsilon$) y el kernel adecuado puede ser compleja y requiere validación cruzada.\n",
    "- Sensible a la escala de los datos, por lo que la estandarización es crítica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generación de datos de ejemplo\n",
    "np.random.seed(42)\n",
    "X = np.sort(5 * np.random.rand(100, 1), axis=0)\n",
    "y = np.sin(X).ravel() + np.random.normal(0, 0.1, X.shape[0])\n",
    "\n",
    "# Dividir en conjunto de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Escalado de los datos\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).ravel()\n",
    "\n",
    "# Definir el modelo SVR\n",
    "svr = SVR(kernel='rbf', C=1.0, epsilon=0.1)\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "svr.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "# Predicciones\n",
    "y_pred_scaled = svr.predict(X_test_scaled)\n",
    "y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
    "\n",
    "# Evaluación del modelo\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# Visualización de los resultados\n",
    "plt.scatter(X_test, y_test, color='red', label='Datos Reales')\n",
    "plt.plot(X_test, y_pred, color='blue', label='Predicciones SVR')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Support Vector Regression')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
