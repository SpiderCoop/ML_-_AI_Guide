{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento y Representacion de texto \n",
    "\n",
    "Es necesario realizar un preprocesamiento adecuado para poder usar modelos de procesamiento de lenguaje natural (NLP). A continuación, se presenta una guía estructurada para realizar este preprocesamiento.\n",
    "\n",
    "## Preprocesamiento del Texto\n",
    "El primer paso es transformar el texto en un formato adecuado para ser procesado por un modelo de aprendizaje automático. Lo cual incluye:\n",
    "\n",
    "- **Tokenización**: Dividir el texto en palabras o tokens.\n",
    "- **Normalización**: Convertir texto a minúsculas, eliminar puntuación y caracteres especiales.\n",
    "- **Lematización o stemming**: Reducir las palabras a su raíz.\n",
    "- **Eliminación de palabras vacías (stopwords)**: Eliminar palabras comunes que no aportan mucho significado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo en Python usando `nltk`:\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "text = \"La clasificación de textos es una tarea importante en el procesamiento de lenguaje natural.\"\n",
    "\n",
    "# Tokenización\n",
    "tokens = word_tokenize(text.lower())\n",
    "\n",
    "# Eliminación de stopwords\n",
    "tokens = [word for word in tokens if word.isalpha() and word not in stopwords.words('spanish')]\n",
    "\n",
    "# Lematización\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representación del Texto\n",
    "Después del preprocesamiento, el texto debe ser transformado en una representación numérica. A continuacion se presentan diferentes tecnicas o algoritmos para realizar esta tarea.\n",
    "\n",
    "### **Bag of Words (BoW)**: \n",
    "Convierte al texto en vectores numéricos de acuerdo a la frecuencia de las palabras encontradas. En otras paabras, se representa al texto como una bolsa de palabras, ignorando el orden pero contando la frecuencia.\n",
    "\n",
    "**Proceso:** \n",
    "Para cada grupo de textos o **corpus**, se crea un vocabulario único con todas las palabras que contiene, despues se crea un vector cuya longitud es igual al tamaño del vocabulario. Cada posición en el vector representa una palabra específica, y el valor en esa posición es la frecuencia con la que aparece la palabra en cada texto.\n",
    "\n",
    "**Limitaciones:** \n",
    "- Ignora la semantica: No tiene en cuenta el significado o contexto de las palabras.\n",
    "- Alta dimensionalidad: Si el vocabulario es grande, los vectores resultantes pueden ser muy largos y dispersos.\n",
    "- Simplicidad: No considera la importancia relativa de las palabras en el corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Term Frequency-Inverse Document Frequency (TF-IDF)**: \n",
    "TF-IDF es una mejora sobre BoW que no solo tiene en cuenta la frecuencia de las palabras en un documento (TF), sino también la importancia de esas palabras en todo el corpus (IDF). La idea principal es reducir la importancia de palabras muy comunes que no son útiles para la clasificación.\n",
    "\n",
    "\n",
    "**Proceso:**\n",
    "1. **Term Frequency (TF)**: Mide la frecuencia de una palabra en un documento específico. Se calcula como el número de veces que una palabra aparece en un documento dividido por el número total de palabras en ese documento.\n",
    "\n",
    "$$\n",
    "TF(t, d) = \\frac{\\text{Número de veces que la palabra } t \\text{ aparece en el documento } d}{\\text{Número total de palabras en el documento } d}\n",
    "$$\n",
    "\n",
    "2. **Inverse Document Frequency (IDF)**: Mide la importancia de una palabra en el corpus. Se calcula como el logaritmo del número total de documentos dividido por el número de documentos que contienen la palabra.\n",
    "\n",
    "$$\n",
    "IDF(t, D) = \\log \\left( \\frac{\\text{Número total de documentos en el corpus } D}{\\text{Número de documentos en los que aparece la palabra } t} \\right)\n",
    "$$\n",
    "\n",
    "3. **TF-IDF**: Es el producto de TF e IDF. Un valor alto de TF-IDF indica que una palabra es importante en un documento pero no aparece frecuentemente en otros documentos.\n",
    "\n",
    "$$\n",
    "TF\\text{-}IDF(t, d, D) = TF(t, d) \\times IDF(t, D)\n",
    "$$\n",
    "\n",
    "**Ventajas:**\n",
    "- **Ponderación inteligente**: Reduce el peso de palabras comunes y aumenta la importancia de términos raros pero relevantes.\n",
    "- **Mejor discriminación**: Funciona mejor que BoW para identificar las palabras más relevantes en un documento.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Word Embeddings**\n",
    "Word Embeddings son representaciones densas de palabras en un espacio continuo, donde las palabras con significados similares tienen representaciones similares. A diferencia de BoW y TF-IDF, Word Embeddings capturan la semántica de las palabras y las relaciones contextuales.\n",
    "\n",
    "**Proceso:**\n",
    "- **Entrenamiento**: Se entrenan en grandes corpus de texto mediante modelos como Word2Vec, GloVe, o FastText. Durante el entrenamiento, las palabras se mapean a un espacio vectorial de baja dimensión (por ejemplo, 100 o 300 dimensiones).\n",
    "- **Representación**: Cada palabra se representa como un vector en este espacio, y la proximidad entre vectores refleja la similitud semántica entre palabras.\n",
    "\n",
    "   Ejemplo de Word Embedding en un espacio de 3 dimensiones:\n",
    "   - \"Rey\": [0.25, 0.8, 0.1]\n",
    "   - \"Reina\": [0.24, 0.79, 0.15]\n",
    "   - \"Hombre\": [0.3, 0.75, 0.2]\n",
    "\n",
    "   En este ejemplo, las palabras \"Rey\" y \"Reina\" estarán más cerca entre sí en el espacio vectorial que \"Rey\" y \"Hombre\".\n",
    "\n",
    "**Ventajas:**\n",
    "- **Captura semántica**: Captura relaciones complejas entre palabras, como sinónimos, analogías, y similitudes contextuales.\n",
    "- **Dimensionalidad reducida**: A diferencia de BoW y TF-IDF, los Word Embeddings tienen una dimensionalidad fija y baja, lo que hace que sean más manejables y eficientes.\n",
    "- **Transferibilidad**: Los Word Embeddings entrenados en un corpus grande pueden transferirse a otros problemas de NLP, facilitando la implementación en tareas específicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ejemplo con `CountVectorizer` y `TfidfVectorizer`:\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "documents = [\n",
    "    \"La clasificación de textos es importante.\",\n",
    "    \"El procesamiento de lenguaje natural es interesante.\",\n",
    "    \"Clasificación de texto con NLP es una tarea común.\"\n",
    "]\n",
    "\n",
    "# Bag of Words\n",
    "vectorizer = CountVectorizer()\n",
    "bow = vectorizer.fit_transform(documents)\n",
    "print(bow.toarray())\n",
    "\n",
    "# TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "print(tfidf.toarray())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
